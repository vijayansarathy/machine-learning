{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzk7iX_CodX6",
    "tags": []
   },
   "source": [
    "# <img align=\"left\" src=\"./images/movie_camera.png\"     style=\" width:40px;  \" > Collaborative Filtering Recommender Systems\n",
    "\n",
    "In this exercise, you will implement collaborative filtering to build a recommender system for movies. \n",
    "\n",
    "# <img align=\"left\" src=\"./images/film_reel.png\"     style=\" width:40px;  \" > Outline\n",
    "- [ 1 - Notation](#1)\n",
    "- [ 2 - Recommender Systems](#2)\n",
    "- [ 3 - Movie ratings dataset](#3)\n",
    "- [ 4 - Collaborative filtering learning algorithm](#4)\n",
    "  - [ 4.1 Collaborative filtering cost function](#4.1)\n",
    "    - [ Exercise 1](#ex01)\n",
    "- [ 5 - Learning movie recommendations](#5)\n",
    "- [ 6 - Recommendations](#6)\n",
    "- [ 7 - Congratulations!](#7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy, math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from recsys_utils import *\n",
    "from numpy import loadtxt\n",
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Recommender Systems <img align=\"left\" src=\"./images/film_rating.png\" style=\" width:40px;  \" >\n",
    "In this lab, you will implement the collaborative filtering learning algorithm and apply it to a dataset of movie ratings.\n",
    "The goal of a collaborative filtering recommender system is to generate two vectors: For each user, a 'parameter vector' that embodies the movie tastes of a user. For each movie, a feature vector of the same size which embodies some description of the movie. The dot product of the two vectors plus the bias term should produce an estimate of the rating the user might give to that movie.\n",
    "\n",
    "The diagram below details how these vectors are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "   <img src=\"./images/ColabFilterLearn.PNG\"  style=\"width:740px;height:250px;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing ratings are provided in matrix form as shown. $Y$ contains ratings; 0.5 to 5 inclusive in 0.5 steps. 0 if the movie has not been rated. $R$ has a 1 where movies have been rated. Movies are in rows, users in columns. Each user has a parameter vector $w^{user}$ and bias. Each movie has a feature vector $x^{movie}$. These vectors are simultaneously learned by using the existing user/movie ratings as training data. One training example is shown above: $\\mathbf{w}^{(1)} \\cdot \\mathbf{x}^{(1)} + b^{(1)} = 4$. It is worth noting that the feature vector $x^{movie}$ must satisfy all the users while the user vector $w^{user}$ must satisfy all the movies. This is the source of the name of this approach - all the users collaborate to generate the rating set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "   <img src=\"./images/ColabFilterUse.PNG\"  style=\"width:640px;height:250px;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the feature vectors and parameters are learned, they can be used to predict how a user might rate an unrated movie. This is shown in the diagram above. The equation is an example of predicting a rating for user one on movie zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will implement the function `cofiCostFunc` that computes the collaborative filtering\n",
    "objective function. After implementing the objective function, you will use a TensorFlow custom training loop to learn the parameters for collaborative filtering. The first step is to detail the data set and data structures that will be used in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-09Hto6odYD"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Movie ratings dataset <img align=\"left\" src=\"./images/film_rating.png\"     style=\" width:40px;  \" >\n",
    "The data set is derived from the [MovieLens \"ml-latest-small\"](https://grouplens.org/datasets/movielens/latest/) dataset.   \n",
    "[F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>]\n",
    "\n",
    "The original dataset has  9000 movies rated by 600 users. The dataset has been reduced in size to focus on movies from the years since 2000. This dataset consists of ratings on a scale of 0.5 to 5 in 0.5 step increments. The reduced dataset has $n_u = 443$ users, and $n_m= 4778$ movies. \n",
    "\n",
    "Below, you will load the movie dataset into the variables $Y$ and $R$.\n",
    "\n",
    "The matrix $Y$ (a  $n_m \\times n_u$ matrix) stores the ratings $y^{(i,j)}$. The matrix $R$ is an binary-valued indicator matrix, where $R(i,j) = 1$ if user $j$ gave a rating to movie $i$, and $R(i,j)=0$ otherwise. \n",
    "\n",
    "Throughout this part of the exercise, you will also be working with the\n",
    "matrices, $\\mathbf{X}$, $\\mathbf{W}$ and $\\mathbf{b}$: \n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\mathbf{x}^{(0)})^T --- \\\\\n",
    "--- (\\mathbf{x}^{(1)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\mathbf{x}^{(n_m-1)})^T --- \\\\\n",
    "\\end{bmatrix} , \\quad\n",
    "\\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\mathbf{w}^{(0)})^T --- \\\\\n",
    "--- (\\mathbf{w}^{(1)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\mathbf{w}^{(n_u-1)})^T --- \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{ b} = \n",
    "\\begin{bmatrix}\n",
    " b^{(0)}  \\\\\n",
    " b^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(n_u-1)} \\\\\n",
    "\\end{bmatrix}\\quad\n",
    "$$ \n",
    "\n",
    "The $i$-th row of $\\mathbf{X}$ corresponds to the\n",
    "feature vector $x^{(i)}$ for the $i$-th movie, and the $j$-th row of\n",
    "$\\mathbf{W}$ corresponds to one parameter vector $\\mathbf{w}^{(j)}$, for the\n",
    "$j$-th user. Both $x^{(i)}$ and $\\mathbf{w}^{(j)}$ are $n$-dimensional\n",
    "vectors. For the purposes of this exercise, you will use $n=10$, and\n",
    "therefore, $\\mathbf{x}^{(i)}$ and $\\mathbf{w}^{(j)}$ have 10 elements.\n",
    "Correspondingly, $\\mathbf{X}$ is a\n",
    "$n_m \\times 10$ matrix and $\\mathbf{W}$ is a $n_u \\times 10$ matrix.\n",
    "\n",
    "We will start by loading the movie ratings dataset to understand the structure of the data.\n",
    "We will load $Y$ and $R$ with the movie dataset.  \n",
    "We'll also load $\\mathbf{X}$, $\\mathbf{W}$, and $\\mathbf{b}$ with pre-computed values. These values will be learned later in the lab, but we'll use pre-computed values to develop the cost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (4778, 443) R (4778, 443)\n",
      "X (4778, 10)\n",
      "W (443, 10)\n",
      "b (1, 443)\n",
      "num_features 10\n",
      "num_movies 4778\n",
      "num_users 443\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "X, W, b, num_movies, num_features, num_users = load_precalc_params_small()\n",
    "Y, R = load_ratings_small()\n",
    "\n",
    "print(\"Y\", Y.shape, \"R\", R.shape)\n",
    "print(\"X\", X.shape)\n",
    "print(\"W\", W.shape)\n",
    "print(\"b\", b.shape)\n",
    "print(\"num_features\", num_features)\n",
    "print(\"num_movies\",   num_movies)\n",
    "print(\"num_users\",    num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bxm1O_wbodYF",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for movie 1 : 3.400 / 5\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# From the matrix, we can compute statistics like average rating.\n",
    "#\n",
    "tsmean =  np.mean(Y[0, R[0, :].astype(bool)])\n",
    "print(f\"Average rating for movie 1 : {tsmean:0.3f} / 5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Collaborative filtering learning algorithm <img align=\"left\" src=\"./images/film_filter.png\"     style=\" width:40px;  \" >\n",
    "\n",
    "Now, you will begin implementing the collaborative filtering learning\n",
    "algorithm. You will start by implementing the objective function. \n",
    "\n",
    "The collaborative filtering algorithm in the setting of movie\n",
    "recommendations considers a set of $n$-dimensional parameter vectors\n",
    "$\\mathbf{x}^{(0)},...,\\mathbf{x}^{(n_m-1)}$, $\\mathbf{w}^{(0)},...,\\mathbf{w}^{(n_u-1)}$ and $b^{(0)},...,b^{(n_u-1)}$, where the\n",
    "model predicts the rating for movie $i$ by user $j$ as\n",
    "$y^{(i,j)} = \\mathbf{w}^{(j)}\\cdot \\mathbf{x}^{(i)} + b^{(i)}$ . Given a dataset that consists of\n",
    "a set of ratings produced by some users on some movies, you wish to\n",
    "learn the parameter vectors $\\mathbf{x}^{(0)},...,\\mathbf{x}^{(n_m-1)},\n",
    "\\mathbf{w}^{(0)},...,\\mathbf{w}^{(n_u-1)}$  and $b^{(0)},...,b^{(n_u-1)}$ that produce the best fit (minimizes\n",
    "the squared error).\n",
    "\n",
    "You will complete the code in cofiCostFunc to compute the cost\n",
    "function for collaborative filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcqg0LJWodYH"
   },
   "source": [
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Collaborative filtering cost function\n",
    "\n",
    "The collaborative filtering cost function is given by\n",
    "$$J({\\mathbf{x}^{(0)},...,\\mathbf{x}^{(n_m-1)},\\mathbf{w}^{(0)},b^{(0)},...,\\mathbf{w}^{(n_u-1)},b^{(n_u-1)}})= \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2\n",
    "+\\underbrace{\n",
    "\\frac{\\lambda}{2}\n",
    "\\sum_{j=0}^{n_u-1}\\sum_{k=0}^{n-1}(\\mathbf{w}^{(j)}_k)^2\n",
    "+ \\frac{\\lambda}{2}\\sum_{i=0}^{n_m-1}\\sum_{k=0}^{n-1}(\\mathbf{x}_k^{(i)})^2\n",
    "}_{regularization}\n",
    "\\tag{1}$$\n",
    "The first summation in (1) is \"for all $i$, $j$ where $r(i,j)$ equals $1$\" and could be written:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2}\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2\n",
    "+\\text{regularization}\n",
    "$$\n",
    "\n",
    "You should now write cofiCostFunc (collaborative filtering cost function) to return this cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "**For loop Implementation:**   \n",
    "Start by implementing the cost function using for loops.\n",
    "Consider developing the cost function in two steps. First, develop the cost function without regularization. A test case that does not include regularization is provided below to test your implementation. Once that is working, add regularization and run the tests that include regularization.  Note that you should be accumulating the cost for user $j$ and movie $i$ only if $R(i,j) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movie was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    nm, nu = Y.shape\n",
    "    f_wb = np.matmul (X,W.T) + b\n",
    "    f_wb_yr = np.square (f_wb - Y) * R\n",
    "    J = 0.5 * (np.sum(f_wb_yr) + lambda_ * (np.linalg.norm(W)**2 + np.linalg.norm(X)**2))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorized Implementation**\n",
    "\n",
    "It is important to create a vectorized implementation to compute $J$, since it will later be called many times during optimization. The linear algebra utilized is not the focus of this series, so the implementation is provided. If you are an expert in linear algebra, feel free to create your version without referencing the code below. \n",
    "\n",
    "Run the code below and verify that it produces the same results as the non-vectorized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost_tf(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # We are using Tensorflow APIs to compute the cost\n",
    "    # We need to do this so that Tensorflow can track these operations when it need to compute gradients using AutoDiff\n",
    "    #\n",
    "    f_wb_yr = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(f_wb_yr**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilaeM8yWodYR"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Learning movie recommendations <img align=\"left\" src=\"./images/film_man_action.png\" style=\" width:40px;  \" >\n",
    "------------------------------\n",
    "\n",
    "After you have finished implementing the collaborative filtering cost\n",
    "function, you can start training your algorithm to make\n",
    "movie recommendations for yourself. \n",
    "\n",
    "In the cell below, you can enter your own movie choices. The algorithm will then make recommendations for you! We have filled out some values according to our preferences, but after you have things working with our choices, you should change this to match your tastes.\n",
    "A list of all movies in the dataset is in the file [movie list](data/small_movie_list.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WJO8Jr0UodYR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 79, 204, 246, 271, 382, 610, 622, 653, 793, 837, 929, 988, 1051, 1150, 2609, 2700, 2716, 2925, 2937]\n",
      "-- Movie ratings by Viji Sarathy ---\n",
      "Rated 4.0 for  Road to El Dorado, The (2000)\n",
      "Rated 4.0 for  X-Men (2000)\n",
      "Rated 3.0 for  Series 7: The Contenders (2001)\n",
      "Rated 5.0 for  Shrek (2001)\n",
      "Rated 1.0 for  Cats & Dogs (2001)\n",
      "Rated 2.0 for  Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\n",
      "Rated 5.0 for  Ghost Ship (2002)\n",
      "Rated 5.0 for  Harry Potter and the Chamber of Secrets (2002)\n",
      "Rated 4.0 for  Lord of the Rings: The Two Towers, The (2002)\n",
      "Rated 5.0 for  Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
      "Rated 2.0 for  Stoked: The Rise and Fall of Gator (2002)\n",
      "Rated 5.0 for  Lord of the Rings: The Return of the King, The (2003)\n",
      "Rated 3.0 for  Eternal Sunshine of the Spotless Mind (2004)\n",
      "Rated 5.0 for  Harry Potter and the Prisoner of Azkaban (2004)\n",
      "Rated 5.0 for  Incredibles, The (2004)\n",
      "Rated 2.0 for  Persuasion (2007)\n",
      "Rated 5.0 for  Toy Story 3 (2010)\n",
      "Rated 3.0 for  Inception (2010)\n",
      "Rated 1.0 for  Louis Theroux: Law & Disorder (2008)\n",
      "Rated 1.0 for  Nothing to Declare (Rien à déclarer) (2010)\n"
     ]
    }
   ],
   "source": [
    "movieList, movieList_df = load_Movie_List_pd()\n",
    "\n",
    "# Initialize my ratings\n",
    "my_ratings = np.zeros(num_movies)          \n",
    "\n",
    "#\n",
    "# Create an array that with ratings given by you.\n",
    "# Based in these ratinbgs, we want the algorithm to predict what other movies you may like\n",
    "# Check the file small_movie_list.csv for id of each movie in our dataset\n",
    "#\n",
    "my_ratings[2700] = 5   # Toy Story 3 (2010)\n",
    "my_ratings[2609] = 2   # Persuasion (2007)\n",
    "my_ratings[29]   = 4   # Road to El Dorado, The (2000)\n",
    "my_ratings[79]   = 4   # X-Men (2000)\n",
    "my_ratings[204]  = 3   # The Contenders (2001)\n",
    "my_ratings[271]  = 1   # Cats & Dogs (2001)\n",
    "my_ratings[610]  = 5   # Ghost Ship (2002)\n",
    "my_ratings[837]  = 2   # The Rise and Fall of Gator (2002)\n",
    "my_ratings[929]  = 5   # Lord of the Rings: The Return of the King, The\n",
    "my_ratings[653]  = 4   # Lord of the Rings: The Two Towers, The (2002)\n",
    "my_ratings[246]  = 5   # Shrek (2001)\n",
    "my_ratings[2716] = 3   # Inception\n",
    "my_ratings[1150] = 5   # Incredibles, The (2004)\n",
    "my_ratings[382]  = 2   # Amelie (Fabuleux destin d'Amélie Poulain, Le)\n",
    "my_ratings[1051] = 5   # Harry Potter and the Prisoner of Azkaban (2004)\n",
    "my_ratings[622]  = 5   # Harry Potter and the Chamber of Secrets (2002)\n",
    "my_ratings[988]  = 3   # Eternal Sunshine of the Spotless Mind (2004)\n",
    "my_ratings[2925] = 1   # Louis Theroux: Law & Disorder (2008)\n",
    "my_ratings[2937] = 1   # Nothing to Declare (Rien à déclarer)\n",
    "my_ratings[793]  = 5   # Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
    "\n",
    "#\n",
    "# Show the IDs of all movies for which you have provided a rating\n",
    "#\n",
    "my_rated = [i for i in range(len(my_ratings)) if my_ratings[i] > 0]\n",
    "print (my_rated)\n",
    "\n",
    "print('-- Movie ratings by Viji Sarathy ---')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0 :\n",
    "        print(f'Rated {my_ratings[i]} for  {movieList_df.loc[i,\"title\"]}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload ratings and add new ratings\n",
    "Y, R = load_ratings_small()\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 100\n",
    "\n",
    "#\n",
    "# Concatenate the ratings that you provided to the given data\n",
    "#\n",
    "Y = np.c_[my_ratings, Y]\n",
    "R = np.c_[(my_ratings != 0).astype(int), R]\n",
    "\n",
    "# Normalize the Dataset\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare to train the model. Initialize the parameters and select the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Useful Values\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 100\n",
    "\n",
    "#\n",
    "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "# W represents the matrix of \"parameters\" for the users\n",
    "# X represents the matrix of \"embeddings\" for the movies (items)\n",
    "# The parameter 'num_features' can be set to any value depending but it is always much smaller than 'num_users' and 'num_movies'\n",
    "#\n",
    "tf.random.set_seed(1234) \n",
    "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the collaborative filtering model. This will learn the parameters $\\mathbf{X}$, $\\mathbf{W}$, and $\\mathbf{b}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations involved in learning $w$, $b$, and $x$ simultaneously do not fall into the typical 'layers' offered in the TensorFlow neural network package.  Consequently, the flow used in Course 2: Model, Compile(), Fit(), Predict(), are not directly applicable. Instead, we can use a custom training loop.\n",
    "\n",
    "Recall from earlier labs the steps of gradient descent.\n",
    "- repeat until convergence:\n",
    "    - compute forward pass\n",
    "    - compute the derivatives of the loss relative to parameters\n",
    "    - update the parameters using the learning rate and the computed derivatives \n",
    "    \n",
    "TensorFlow has the marvelous capability of calculating the derivatives for you. This is shown below. Within the `tf.GradientTape()` section, operations on Tensorflow Variables are tracked. When `tape.gradient()` is later called, it will return the gradient of the loss relative to the tracked variables. The gradients can then be applied to the parameters using an optimizer. \n",
    "This is a very brief introduction to a useful feature of TensorFlow and other machine learning frameworks. Further information can be found by investigating \"custom training loops\" within the framework of interest.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving computed weights and features to ./data/weights-and-features-tf.npz\n",
      "Extension horovod.torch has not been built: /usr/local/lib/python3.8/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-38-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2023-04-27 19:30:23.703 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:234 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-27 19:30:24.098 tensorflow-2-6-cpu-py-ml-t3-medium-9169b2e75617c45c79c40579f6a8:234 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Training loss at iteration 0: 2321300.0,  2321300.0\n",
      "Training loss at iteration 20: 136174.9,  136174.9\n",
      "Training loss at iteration 40: 51870.9,  51870.9\n",
      "Training loss at iteration 60: 24603.1,  24603.1\n",
      "Training loss at iteration 80: 13633.0,  13633.0\n",
      "Training loss at iteration 100: 8489.3,  8489.3\n",
      "Training loss at iteration 120: 5808.9,  5808.9\n",
      "Training loss at iteration 140: 4312.7,  4312.7\n",
      "Training loss at iteration 160: 3436.3,  3436.3\n",
      "Training loss at iteration 180: 2903.1,  2903.1\n",
      "Training loss at iteration 200: 2567.6,  2567.6\n",
      "Training loss at iteration 220: 2349.7,  2349.7\n",
      "Training loss at iteration 240: 2203.9,  2203.9\n",
      "Training loss at iteration 260: 2103.4,  2103.4\n",
      "Training loss at iteration 280: 2032.2,  2032.2\n",
      "Training loss at iteration 300: 1980.5,  1980.5\n",
      "Training loss at iteration 320: 1941.9,  1941.9\n",
      "Training loss at iteration 340: 1912.5,  1912.5\n",
      "Training loss at iteration 360: 1889.6,  1889.6\n",
      "Training loss at iteration 380: 1871.4,  1871.4\n",
      "Training loss at iteration 400: 1856.7,  1856.7\n",
      "Training loss at iteration 420: 1844.7,  1844.7\n",
      "Training loss at iteration 440: 1834.7,  1834.7\n",
      "Training loss at iteration 460: 1826.3,  1826.3\n",
      "Training loss at iteration 480: 1819.2,  1819.2\n"
     ]
    }
   ],
   "source": [
    "outfile=\"./data/weights-and-features-tf.npz\"\n",
    "print (f\"Saving computed weights and features to {outfile}\")\n",
    "np.savez (outfile, weights=W, bias=b, features=X)\n",
    "              \n",
    "iterations = 500\n",
    "lambda_ = 1\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlow’s GradientTape\n",
    "    # to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = compute_cost_tf(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient(cost_value, [X,W,b])\n",
    "\n",
    "    W_np = W.numpy()\n",
    "    X_np = X.numpy()\n",
    "    b_np = b.numpy()\n",
    "    J = compute_cost_tf(X_np, W_np, b_np, Ynorm, R, lambda_)\n",
    "    \n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f},  {J:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, W, b, Y, R, RU, RM, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movie was rated by the j-th user\n",
    "      RU (1, num_users)                    : matrix of no.of ratings given by each user\n",
    "      RM (1, num_movies)                   : matrix of no.of ratings given to each movie\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (num_features,num_users)):  The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_dx (ndarray (num_features,num_movies)): The gradient of the cost w.r.t. the features x.\n",
    "      dj_db (1, num_users):                      The gradient of the cost w.r.t. the parameter b.\n",
    "\n",
    "    \"\"\"\n",
    "    nm, nu = Y.shape\n",
    "    delta = (np.matmul (X,W.T) + b - Y) * R\n",
    "\n",
    "    dj_dw = np.divide (np.matmul(X.T,delta), RU) + np.divide ((lambda_*W.T), RU)\n",
    "    dj_dx = np.divide (np.transpose(np.matmul(delta,W)), RM) + np.divide ((lambda_*X.T), RM)\n",
    "    dj_db = np.divide (np.sum(delta,axis=0),RU)\n",
    "\n",
    "    return dj_dw, dj_dx, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Save the old results\n",
    "#\n",
    "W_old = W.numpy()\n",
    "X_old = X.numpy()\n",
    "b_old = b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1823218.30   \n",
      "Saving computed weights and features at    0 to ./data/weights-and-features-0.npz\n",
      "Saving computed weights and features at   20 to ./data/weights-and-features-20.npz\n",
      "Saving computed weights and features at   40 to ./data/weights-and-features-40.npz\n",
      "Saving computed weights and features at   60 to ./data/weights-and-features-60.npz\n",
      "Saving computed weights and features at   80 to ./data/weights-and-features-80.npz\n",
      "Iteration  100: Cost 145102.74   \n",
      "Saving computed weights and features at  100 to ./data/weights-and-features-100.npz\n",
      "Saving computed weights and features at  120 to ./data/weights-and-features-120.npz\n",
      "Saving computed weights and features at  140 to ./data/weights-and-features-140.npz\n",
      "Saving computed weights and features at  160 to ./data/weights-and-features-160.npz\n",
      "Saving computed weights and features at  180 to ./data/weights-and-features-180.npz\n",
      "Iteration  200: Cost 86162.49   \n",
      "Saving computed weights and features at  200 to ./data/weights-and-features-200.npz\n",
      "Saving computed weights and features at  220 to ./data/weights-and-features-220.npz\n",
      "Saving computed weights and features at  240 to ./data/weights-and-features-240.npz\n",
      "Saving computed weights and features at  260 to ./data/weights-and-features-260.npz\n",
      "Saving computed weights and features at  280 to ./data/weights-and-features-280.npz\n",
      "Iteration  300: Cost 64237.37   \n",
      "Saving computed weights and features at  300 to ./data/weights-and-features-300.npz\n",
      "Saving computed weights and features at  320 to ./data/weights-and-features-320.npz\n",
      "Saving computed weights and features at  340 to ./data/weights-and-features-340.npz\n",
      "Saving computed weights and features at  360 to ./data/weights-and-features-360.npz\n",
      "Saving computed weights and features at  380 to ./data/weights-and-features-380.npz\n",
      "Iteration  400: Cost 52081.88   \n",
      "Saving computed weights and features at  400 to ./data/weights-and-features-400.npz\n",
      "Saving computed weights and features at  420 to ./data/weights-and-features-420.npz\n",
      "Saving computed weights and features at  440 to ./data/weights-and-features-440.npz\n",
      "Saving computed weights and features at  460 to ./data/weights-and-features-460.npz\n",
      "Saving computed weights and features at  480 to ./data/weights-and-features-480.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "# Reload ratings and add new ratings\n",
    "#\n",
    "Y, R = load_ratings_small()\n",
    "\n",
    "#\n",
    "# Concatenate the ratings that you provided to the given data\n",
    "#\n",
    "Y = np.c_[my_ratings, Y]\n",
    "R = np.c_[(my_ratings != 0).astype(int), R]\n",
    "RU = np.sum(R,axis=0,keepdims=True)\n",
    "RM = np.transpose(np.sum(R,axis=1,keepdims=True))\n",
    "\n",
    "#\n",
    "# Normalize the Dataset\n",
    "#\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)\n",
    "\n",
    "#\n",
    "#  Useful Parameters\n",
    "#\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 100\n",
    "\n",
    "#\n",
    "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "#\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W_tf = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X_tf = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b_tf = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "#\n",
    "# Convert the TensorFlow arrays to Numpy arrays\n",
    "#\n",
    "W = W_tf.numpy()\n",
    "X = X_tf.numpy()\n",
    "b = b_tf.numpy()\n",
    "\n",
    "#\n",
    "# Read the saved weights and features from the file\n",
    "#\n",
    "start_iter = 0\n",
    "#infile=\"./data/weights-and-features-\" + str(start_iter) + \".npz\"\n",
    "#print (f\"Loading computed weights and features at {start_iter:4d} from {infile}\")\n",
    "#npzfile = np.load(infile)\n",
    "#W = npzfile[\"weights\"]\n",
    "#b = npzfile[\"bias\"]\n",
    "#X = npzfile[\"features\"]\n",
    "\n",
    "\n",
    "iterations = 500\n",
    "lambda_ = 1\n",
    "learning_rate = 1e-2\n",
    "for iter in range(iterations):\n",
    "    # Calculate the gradient \n",
    "    dj_dw, dj_dx, dj_db = compute_gradient(X, W, b, Ynorm, R, RU, RM, lambda_)\n",
    "    \n",
    "    # Update Parameters using w, b, x, alpha and gradient\n",
    "    W = W - learning_rate * np.transpose (dj_dw)            \n",
    "    X = X - learning_rate * np.transpose (dj_dx)            \n",
    "    b = b - learning_rate * dj_db \n",
    "    \n",
    "    # Compute the cost J at each iteration\n",
    "    J = compute_cost(X, W, b, Ynorm, R, lambda_)\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"Iteration {iter:4d}: Cost {J:8.2f}   \")\n",
    "\n",
    "    # After every 1000 iterations, save the computed weights & features to a distinct file\n",
    "    total_iter = start_iter + iter\n",
    "    if total_iter % 20 == 0:\n",
    "        outfile=\"./data/weights-and-features-\" + str(total_iter) + \".npz\"\n",
    "        print (f\"Saving computed weights and features at {total_iter:4d} to {outfile}\")\n",
    "        np.savez (outfile, weights=W, bias=b, features=X)\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSzUL7eQodYS"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Recommendations\n",
    "Below, we compute the ratings for all the movies and users and display the movies that are recommended. These are based on the movies and ratings entered as `my_ratings[]` above. To predict the rating of movie $i$ for user $j$, you compute $\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}$. This can be computed for all ratings using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ns266wKtodYT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading computed weights and features at 10000 from ./data/weights-and-features-10000.npz\n",
      "Predicting rating 2.84 for movie Lovely & Amazing (2001)\n",
      "Predicting rating 3.31 for movie Cemetery Junction (2010)\n",
      "Predicting rating 3.81 for movie Trials of Henry Kissinger, The (2002)\n",
      "Predicting rating 2.91 for movie Number 23, The (2007)\n",
      "Predicting rating 4.48 for movie Doctor Who: The Time of the Doctor (2013)\n",
      "Predicting rating 2.02 for movie House of Mirth, The (2000)\n",
      "Predicting rating 2.79 for movie Batman: Assault on Arkham (2014)\n",
      "Predicting rating 3.34 for movie Pride & Prejudice (2005)\n",
      "Predicting rating 3.43 for movie Moana (2016)\n",
      "Predicting rating 2.31 for movie Someone Marry Barry (2014)\n",
      "Predicting rating 2.79 for movie Ip Man 2 (2010)\n",
      "Predicting rating 0.21 for movie Ben-hur (2016)\n",
      "Predicting rating 3.59 for movie Notes on a Scandal (2006)\n",
      "Predicting rating 3.79 for movie Down Terrace (2009)\n",
      "Predicting rating 2.18 for movie Clash of the Titans (2010)\n",
      "Predicting rating 3.83 for movie Fruitvale Station (2013)\n",
      "Predicting rating 4.31 for movie Red Lights (Feux rouges) (2004)\n",
      "Predicting rating 0.81 for movie Big Mommas: Like Father, Like Son (2011)\n",
      "Predicting rating 4.29 for movie Redline (2009)\n",
      "Predicting rating 2.79 for movie Imaginary Heroes (2004)\n",
      "Predicting rating 2.88 for movie Grindhouse (2007)\n",
      "Predicting rating 1.97 for movie Serbian Film, A (Srpski film) (2010)\n",
      "Predicting rating 3.77 for movie Barbarian Invasions, The (Les invasions barbares) (2003)\n",
      "\n",
      "\n",
      "Original vs Predicted ratings:\n",
      "\n",
      "Original 4.0, Predicted 3.89 for Road to El Dorado, The (2000)\n",
      "Original 4.0, Predicted 3.97 for X-Men (2000)\n",
      "Original 3.0, Predicted 2.88 for Series 7: The Contenders (2001)\n",
      "Original 5.0, Predicted 4.93 for Shrek (2001)\n",
      "Original 1.0, Predicted 1.32 for Cats & Dogs (2001)\n",
      "Original 2.0, Predicted 2.15 for Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\n",
      "Original 5.0, Predicted 4.53 for Ghost Ship (2002)\n",
      "Original 5.0, Predicted 4.94 for Harry Potter and the Chamber of Secrets (2002)\n",
      "Original 4.0, Predicted 4.04 for Lord of the Rings: The Two Towers, The (2002)\n",
      "Original 5.0, Predicted 4.92 for Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
      "Original 2.0, Predicted 1.95 for Stoked: The Rise and Fall of Gator (2002)\n",
      "Original 5.0, Predicted 4.92 for Lord of the Rings: The Return of the King, The (2003)\n",
      "Original 3.0, Predicted 3.02 for Eternal Sunshine of the Spotless Mind (2004)\n",
      "Original 5.0, Predicted 4.89 for Harry Potter and the Prisoner of Azkaban (2004)\n",
      "Original 5.0, Predicted 4.93 for Incredibles, The (2004)\n",
      "Original 2.0, Predicted 2.17 for Persuasion (2007)\n",
      "Original 5.0, Predicted 4.90 for Toy Story 3 (2010)\n",
      "Original 3.0, Predicted 3.00 for Inception (2010)\n",
      "Original 1.0, Predicted 1.36 for Louis Theroux: Law & Disorder (2008)\n",
      "Original 1.0, Predicted 1.27 for Nothing to Declare (Rien à déclarer) (2010)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the saved weights and features from the file\n",
    "#\n",
    "start_iter = 10000\n",
    "infile=\"./data/weights-and-features-\" + str(start_iter) + \".npz\"\n",
    "print (f\"Loading computed weights and features at {start_iter:4d} from {infile}\")\n",
    "npzfile = np.load(infile)\n",
    "W = npzfile[\"weights\"]\n",
    "b = npzfile[\"bias\"]\n",
    "X = npzfile[\"features\"]\n",
    "\n",
    "\n",
    "# Make a prediction using trained weights and biases\n",
    "#p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "p = np.matmul(X, W.T) + b\n",
    "\n",
    "#\n",
    "# When using mean normalization, we subtract the mean rating for each movie from all its ratings\n",
    "# Before making predictions, we will have to restore these values to the original values.\n",
    "#\n",
    "pm = p + Ymean\n",
    "\n",
    "user_id = 0\n",
    "user_predictions = pm[:,user_id]\n",
    "user_ratings = Y[:,user_id]\n",
    "\n",
    "#\n",
    "# sort predictions in descending order\n",
    "#\n",
    "indices = tf.argsort(user_predictions, direction='DESCENDING')\n",
    "num_movies, num_users = Y.shape\n",
    "\n",
    "#\n",
    "# Predict ratings for 50 random movies \n",
    "#\n",
    "for i in range(25):\n",
    "    movie_index = np.random.randint(0,num_movies)\n",
    "    j = indices[movie_index]\n",
    "    if j not in my_rated:\n",
    "        print(f'Predicting rating {user_predictions[j]:0.2f} for movie {movieList[j]}')\n",
    "\n",
    "        \n",
    "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
    "for i in range(len(user_ratings)):\n",
    "    if user_ratings[i] > 0:\n",
    "        print(f'Original {user_ratings[i]}, Predicted {user_predictions[i]:0.2f} for {movieList[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, additional information can be utilized to enhance our predictions. Above, the predicted ratings for the first few hundred movies lie in a small range. We can augment the above by selecting from those top movies, movies that have high average ratings and movies with more than 20 ratings. This section uses a [Pandas](https://pandas.pydata.org/) data frame which has many handy sorting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>mean rating</th>\n",
       "      <th>number of ratings</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>4.620010</td>\n",
       "      <td>4.155914</td>\n",
       "      <td>93</td>\n",
       "      <td>Snatch (2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>4.924036</td>\n",
       "      <td>4.118919</td>\n",
       "      <td>185</td>\n",
       "      <td>Lord of the Rings: The Return of the King, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>4.896067</td>\n",
       "      <td>4.109091</td>\n",
       "      <td>55</td>\n",
       "      <td>Toy Story 3 (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>4.346708</td>\n",
       "      <td>4.106061</td>\n",
       "      <td>198</td>\n",
       "      <td>Lord of the Rings: The Fellowship of the Ring,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>4.446866</td>\n",
       "      <td>3.989362</td>\n",
       "      <td>47</td>\n",
       "      <td>Harry Potter and the Deathly Hallows: Part 1 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3283</th>\n",
       "      <td>4.498875</td>\n",
       "      <td>3.982143</td>\n",
       "      <td>28</td>\n",
       "      <td>Argo (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>4.511342</td>\n",
       "      <td>3.960993</td>\n",
       "      <td>141</td>\n",
       "      <td>Finding Nemo (2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>4.893116</td>\n",
       "      <td>3.913978</td>\n",
       "      <td>93</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4.928474</td>\n",
       "      <td>3.867647</td>\n",
       "      <td>170</td>\n",
       "      <td>Shrek (2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>4.411047</td>\n",
       "      <td>3.862069</td>\n",
       "      <td>58</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>4.929309</td>\n",
       "      <td>3.836000</td>\n",
       "      <td>125</td>\n",
       "      <td>Incredibles, The (2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>4.381702</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>18</td>\n",
       "      <td>127 Hours (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>4.922832</td>\n",
       "      <td>3.778523</td>\n",
       "      <td>149</td>\n",
       "      <td>Pirates of the Caribbean: The Curse of the Bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>4.491395</td>\n",
       "      <td>3.761682</td>\n",
       "      <td>107</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (a.k.a. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>4.944224</td>\n",
       "      <td>3.598039</td>\n",
       "      <td>102</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (2002)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pred  mean rating  number of ratings  \\\n",
       "155   4.620010     4.155914                 93   \n",
       "929   4.924036     4.118919                185   \n",
       "2700  4.896067     4.109091                 55   \n",
       "393   4.346708     4.106061                198   \n",
       "2804  4.446866     3.989362                 47   \n",
       "3283  4.498875     3.982143                 28   \n",
       "773   4.511342     3.960993                141   \n",
       "1051  4.893116     3.913978                 93   \n",
       "246   4.928474     3.867647                170   \n",
       "1930  4.411047     3.862069                 58   \n",
       "1150  4.929309     3.836000                125   \n",
       "2794  4.381702     3.833333                 18   \n",
       "793   4.922832     3.778523                149   \n",
       "366   4.491395     3.761682                107   \n",
       "622   4.944224     3.598039                102   \n",
       "\n",
       "                                                  title  \n",
       "155                                       Snatch (2000)  \n",
       "929   Lord of the Rings: The Return of the King, The...  \n",
       "2700                                 Toy Story 3 (2010)  \n",
       "393   Lord of the Rings: The Fellowship of the Ring,...  \n",
       "2804  Harry Potter and the Deathly Hallows: Part 1 (...  \n",
       "3283                                        Argo (2012)  \n",
       "773                                 Finding Nemo (2003)  \n",
       "1051    Harry Potter and the Prisoner of Azkaban (2004)  \n",
       "246                                        Shrek (2001)  \n",
       "1930   Harry Potter and the Order of the Phoenix (2007)  \n",
       "1150                            Incredibles, The (2004)  \n",
       "2794                                   127 Hours (2010)  \n",
       "793   Pirates of the Caribbean: The Curse of the Bla...  \n",
       "366   Harry Potter and the Sorcerer's Stone (a.k.a. ...  \n",
       "622      Harry Potter and the Chamber of Secrets (2002)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter=(movieList_df[\"number of ratings\"] > 10)\n",
    "movieList_df[\"pred\"] = user_predictions\n",
    "movieList_df = movieList_df.reindex(columns=[\"pred\", \"mean rating\", \"number of ratings\", \"title\"])\n",
    "movieList_df.loc[indices[:300]].loc[filter].sort_values(\"mean rating\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
