{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network with Bells & Whistles\n",
    "\n",
    "This is a complete example of building a deep neural network with implementations of forward and backward propgation.\n",
    "The implementation does not use any frameworks like TensorFlow.\n",
    "It is just plain Python implementations of all the components of a DNN\n",
    "Other elements such as regularization, initialization, normalization, and using momentum etc. have all been incorporated as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (59.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipympl in /opt/conda/lib/python3.7/site-packages (0.9.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from ipympl) (9.4.0)\n",
      "Requirement already satisfied: matplotlib<4,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from ipympl) (3.5.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from ipympl) (1.21.6)\n",
      "Requirement already satisfied: ipython<9 in /opt/conda/lib/python3.7/site-packages (from ipympl) (7.34.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /opt/conda/lib/python3.7/site-packages (from ipympl) (8.0.6)\n",
      "Requirement already satisfied: traitlets<6 in /opt/conda/lib/python3.7/site-packages (from ipympl) (5.9.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (59.3.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (3.0.3)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (4.4.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (0.18.2)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython<9->ipympl) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (5.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=3.4.0->ipympl) (20.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib<4,>=3.4.0->ipympl) (1.14.0)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (7.4.9)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl) (0.1.8)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (25.0.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (4.12.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install ipympl\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Perform randomized initialization of weights & bias parameters\n",
    "# The choice of initialization method could be \"He\", \"Xavier\" or none\n",
    "#\n",
    "def initialize_parameters_momentum_rmsprop(layer_dims, method=\"he\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    momentum = {}\n",
    "    rmsprop = {}\n",
    "    \n",
    "    # total number of layers in the network, including the input layer\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    #\n",
    "    # Initialize the matrix comprising weights (W) and bias values (b) for each layer.\n",
    "    # Note that the input layer does not count as a hidden layer\n",
    "    # Employ a specified initialization which uses a scaling factor based on the number of units in the previous layer\n",
    "    # n_h(i) represents the number of units in hidden layer i\n",
    "    # Dimension of W matrix for layer i = n_h(i) x n_h(i-1), where n_h(i) represents the number of units in hidden layer i\n",
    "    # Dimension of b vector for layer i = n_h(i);\n",
    "    #\n",
    "    print (\"Initializing parameters with '{}' method\".format(method))\n",
    "    for l in range(1, L):\n",
    "        layer_dims_current = layer_dims[l] \n",
    "        layer_dims_previous = layer_dims[l-1]\n",
    "        \n",
    "        scaling_factor = 0.01\n",
    "        if method == \"he\":\n",
    "            scaling_factor = np.sqrt(2.0/layer_dims_previous)\n",
    "        elif method == \"xavier\":\n",
    "            scaling_factor = np.sqrt(1.0/layer_dims_previous)\n",
    "        \n",
    "        parameters[\"W\" + str(l)] = np.random.randn (layer_dims_current, layer_dims_previous) * scaling_factor;\n",
    "        parameters[\"b\" + str(l)] = np.zeros ((layer_dims_current, 1));\n",
    "        \n",
    "        momentum[\"vdW\" + str(l)] = np.zeros ((layer_dims_current, layer_dims_previous));\n",
    "        momentum[\"vdb\" + str(l)] = np.zeros ((layer_dims_current, 1));\n",
    "        \n",
    "        rmsprop[\"sdW\" + str(l)] = np.zeros ((layer_dims_current, layer_dims_previous));\n",
    "        rmsprop[\"sdb\" + str(l)] = np.zeros ((layer_dims_current, 1));        \n",
    "        \n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters, momentum, rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    parameter_cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass \n",
    "    \"\"\"\n",
    "    #\n",
    "    # The output for a given layer are computed using the weights/bias parameters for that layer \n",
    "    # and the activation of the previous layer\n",
    "    #\n",
    "    Z = np.matmul (W, A) + b;\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    parameter_cache = (A, W, b)\n",
    "    return Z, parameter_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    all_cache -- a python dictionary containing \"parameter_cache\" and \"output_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # First, compute the output for a given layer using forward propagation step.\n",
    "    # Then, compute the activation for that layer using either sigmoid or RelU function\n",
    "    # Store the weights W & B, activation and the output for the layer in a cache\n",
    "    # Inputs: \"A_prev, W, b\". \n",
    "    # Outputs: \"A, output_cache\".\n",
    "    #\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, parameter_cache = linear_forward(A_prev, W, b)\n",
    "        A, output_cache = sigmoid(Z);\n",
    "    elif activation == \"relu\":\n",
    "        Z, parameter_cache = linear_forward(A_prev, W, b);\n",
    "        A, output_cache = relu(Z);\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    all_cache = (parameter_cache, output_cache)\n",
    "\n",
    "    return A, all_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, nlayers, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    all_caches = []\n",
    "    A = X\n",
    "    L = nlayers\n",
    "    \n",
    "    #\n",
    "    # Go through each hidden layer and perform linear propagation\n",
    "    # Note that for all hidden layers, we are using RelU function to compute the activation\n",
    "    # As we are doing binary classification, we are using sigmoid activation for the final layer  \n",
    "    # Cache the results from each layer\n",
    "    #\n",
    "    AL = None;\n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A;\n",
    "        W = parameters[\"W\" + str(l)];\n",
    "        b = parameters[\"b\" + str(l)];\n",
    "        \n",
    "        if (l == L):\n",
    "            AL, all_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\");\n",
    "        else:\n",
    "            A, all_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\");\n",
    "        \n",
    "        all_caches.append(all_cache);\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, all_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cross-entropy cost function\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply (Y, np.log(AL)) + np.multiply ((1-Y), np.log(1-AL));\n",
    "    cost = - np.sum(logprobs)/m;  \n",
    "    \n",
    "    # Cost is a scalar value\n",
    "    # Use np.squeeze which removes single-dimensional entries from the shape of an array.\n",
    "    cost = np.squeeze(cost)     \n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, nlayers, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. \n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    L = nlayers \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(AL, Y) \n",
    "    \n",
    "    L2_regularization_cost = 0.0\n",
    "    for l in range(1, L+1):\n",
    "        W = parameters['W' + str(l)];\n",
    "        L2_regularization_cost = L2_regularization_cost + (np.sum(np.square(W)))\n",
    "    \n",
    "    L2_regularization_cost = lambd * L2_regularization_cost / (2*m);\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, parameter_cache, lambd = 0.0):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = parameter_cache;\n",
    "    m = A_prev.shape[1];\n",
    "\n",
    "    dW = np.matmul (dZ, A_prev.T)/m + (lambd/m) * W;\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m;\n",
    "    dA_prev = np.matmul (W.T, dZ);\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape);\n",
    "    assert (dW.shape == W.shape);\n",
    "    assert (db.shape == b.shape);\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, all_cache, activation, lambd = 0.0):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    parameter_cache, output_cache = all_cache\n",
    "    \n",
    "    dZ = None;\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward (dA, output_cache);\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward (dA, output_cache);        \n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, parameter_cache, lambd);     \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, nlayers, all_caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients\n",
    "             gradients[\"dA\" + str(l)] = ... \n",
    "             gradients[\"dW\" + str(l)] = ...\n",
    "             grgradientsads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = nlayers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    #\n",
    "    # First, compute the change in cost function due to changes in the input to the last layer => dZL\n",
    "    # Next, compute dAL which is what we need to get started with back propagation.\n",
    "    #\n",
    "    dZL = AL - Y;\n",
    "    dAL = np.divide (dZL, np.multiply(AL, (1 - AL)));\n",
    "    \n",
    "    #\n",
    "    # Lth hidden layer (SIGMOID -> LINEAR) gradients. \n",
    "    # Inputs: \"dAL, current_cache\". \n",
    "    # Outputs: \"gradients[\"dAL-1\"], gradients[\"dWL\"], gradients[\"dbL\"]\n",
    "    #\n",
    "    current_cache = all_caches[L-1];\n",
    "    dA = dAL\n",
    "    gradients [\"dA\" + str(L-1)], \\\n",
    "    gradients [\"dW\" + str(L)], \\\n",
    "    gradients [\"db\" + str(L)] = linear_activation_backward(dA, current_cache, \"sigmoid\");\n",
    "    \n",
    "    #\n",
    "    # From the (L-1)th layer until the input layer,all layers are using RelU activation\n",
    "    # Hence, we can implement linear-activation backward step for these layers in a loop\n",
    "    #\n",
    "    for l in reversed(range(L-1)):\n",
    "        #\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"gradients[\"dA\" + str(l + 1)], current_cache\". \n",
    "        # Outputs: \"gradients[\"dA\" + str(l)], gradients[\"dW\" + str(l + 1)], gradients[\"db\" + str(l + 1)] \n",
    "        #\n",
    "        current_cache = all_caches[l];\n",
    "        dA = gradients[\"dA\" + str(l + 1)];\n",
    "        gradients[\"dA\" + str(l)], \\\n",
    "        gradients[\"dW\" + str(l + 1)], \\\n",
    "        gradients[\"db\" + str(l + 1)] = linear_activation_backward(dA, current_cache, \"relu\");\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_parameters_adam (nlayers, parameters, gradients, momentum, rmsprop,\n",
    "                            t, learning_rate, beta1, beta2, lambd, m):\n",
    "    L = nlayers \n",
    "    decay_factor = 1 - (learning_rate*lambd/m);\n",
    "    epsilon = 1.0e-08\n",
    "    bias_correction = t;\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        # Retrieve W & b from the dictionary \"parameters\"\n",
    "        W = parameters[\"W\" + str(l)];\n",
    "        b = parameters[\"b\" + str(l)];\n",
    "        \n",
    "        # Retrieve dW & db from the dictionary \"gradients\"\n",
    "        dW = gradients[\"dW\" + str(l)];\n",
    "        db = gradients[\"db\" + str(l)];\n",
    "        \n",
    "        # Retrieve vdW & vdb from the dictionary \"momentum\"\n",
    "        vdW = momentum[\"vdW\" + str(l)];\n",
    "        vdb = momentum[\"vdb\" + str(l)];        \n",
    "        \n",
    "        # Retrieve sdW & sdb from the dictionary \"rmsprop\"\n",
    "        sdW = rmsprop[\"sdW\" + str(l)];\n",
    "        sdb = rmsprop[\"sdb\" + str(l)];   \n",
    "        \n",
    "        # Compute the new momentum terms\n",
    "        vdW = beta1 * vdW + (1 - beta1) * dW\n",
    "        vdb = beta1 * vdb + (1 - beta1) * db\n",
    "        vdW_corrected = vdW/(1 - beta1**bias_correction);\n",
    "        vdb_corrected = vdb/(1 - beta1**bias_correction);\n",
    "        \n",
    "        # Compute the new rmsprop terms\n",
    "        sdW = beta2 * sdW + (1 - beta2) * np.multiply(dW,dW)\n",
    "        sdb = beta2 * sdb + (1 - beta2) * np.multiply(db,db)\n",
    "        sdW_corrected = sdW/(1 - beta2**bias_correction);\n",
    "        sdb_corrected = sdb/(1 - beta2**bias_correction);\n",
    "\n",
    "        #\n",
    "        # Update rule for each parameter\n",
    "        # Notice how regularization factors into the updates as a decay parameter, de-emphasizing the values of W & b.\n",
    "        # Notice how momentum & rmsprop factors into the updates by using the exponential averages of gradients vdW/sdW & vdb/sdb instead of dW & db\n",
    "        #\n",
    "        W = decay_factor * W - learning_rate * vdW_corrected/np.sqrt(sdW_corrected + epsilon);\n",
    "        b = decay_factor * b - learning_rate * vdb_corrected/np.sqrt(sdb_corrected + epsilon);\n",
    "    \n",
    "        parameters[\"W\" + str(l)] = W\n",
    "        parameters[\"b\" + str(l)] = b\n",
    "        \n",
    "        momentum[\"vdW\" + str(l)] = vdW\n",
    "        momentum[\"vdb\" + str(l)] = vdb\n",
    "        \n",
    "        rmsprop[\"sdW\" + str(l)] = sdW\n",
    "        rmsprop[\"sdb\" + str(l)] = sdb        \n",
    "                \n",
    "    return parameters, momentum, rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_parameters (nlayers, parameters, gradients, learning_rate, lambd, m):\n",
    "    L = nlayers \n",
    "    decay_factor = 1 - (learning_rate*lambd/m);\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        # Retrieve W & b from the dictionary \"parameters\"\n",
    "        W = parameters[\"W\" + str(l)];\n",
    "        b = parameters[\"b\" + str(l)];\n",
    "        \n",
    "        # Retrieve dW & db from the dictionary \"gradients\"\n",
    "        dW = gradients[\"dW\" + str(l)];\n",
    "        db = gradients[\"db\" + str(l)];\n",
    "        \n",
    "        #\n",
    "        # Update rule for each parameter\n",
    "        # Notice how regularization factors into the updates as a decay parameter, de-emphasizing the values of W & b.\n",
    "        #\n",
    "        W = decay_factor * W - learning_rate * dW\n",
    "        b = decay_factor * b - learning_rate * db\n",
    "    \n",
    "        parameters[\"W\" + str(l)] = W\n",
    "        parameters[\"b\" + str(l)] = b\n",
    "                \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, \n",
    "                  layers_dims, num_iterations = 2501,\n",
    "                  learning_rate = 0.0075, beta1 = 0.9, beta2 = 0.999, lambd = 1000.,              \n",
    "                  init_method = \"he\",\n",
    "                  print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of features, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []        \n",
    "    m = Y.shape[1]\n",
    "    t = 0                            \n",
    "       \n",
    "    #\n",
    "    # Parameters initialization.\n",
    "    # Pay attention to how the parameters are initialized. This has a BIG effect on how the model performs.\n",
    "    # We have to randomize the parameter values across different layers and not choose the same values for all layers\n",
    "    #\n",
    "    parameters, momentum, rmsprop = initialize_parameters_momentum_rmsprop (layers_dims, method=init_method);\n",
    "    \n",
    "    #\n",
    "    # Gradient Descent Loop\n",
    "    #\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Increment a separate counter for Adam to handle mini-batch which will have an outer loop\n",
    "        t = t + 1\n",
    "        \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, all_caches = L_model_forward(X, nlayers, parameters);\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost_with_regularization(AL, Y, nlayers, parameters, lambd)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        gradients = L_model_backward(AL, Y, nlayers, all_caches);\n",
    " \n",
    "        # Update parameters.\n",
    "        #parameters = update_parameters (nlayers, parameters, gradients, learning_rate, lambd, m)\n",
    "        parameters, momentum, rmsprop = update_parameters_adam (nlayers, parameters, gradients, momentum, rmsprop,\n",
    "                                                                t, learning_rate, beta1, beta2, lambd, m)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    #\n",
    "    # plot the cost\n",
    "    #\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_features(X):\n",
    "    mu = np.mean(X, axis=0)                 \n",
    "    sigma  = np.std(X, axis=0)                  \n",
    "    X_norm = (X - mu) / sigma      \n",
    "    return (X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 209\n",
      "Training set shape = (209, 64, 64, 3)\n",
      "Height/Width of each image = 64\n",
      "Test set size = 50\n",
      "Number of labels = (1, 209)\n",
      "(209, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Read the original dataset\n",
    "#\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset();\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "print (\"Training set size = \" + str(m_train))\n",
    "print (\"Training set shape = \" + str(train_set_x_orig.shape));\n",
    "print (\"Height/Width of each image = \" + str(num_px))\n",
    "print (\"Test set size = \" + str(m_test))\n",
    "print (\"Number of labels = \" + str(train_set_y.shape));\n",
    "print (train_set_x_orig.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Training set size = 209\n",
      "Flattened Training set shape = (209, 12288)\n",
      "Flattened Test set size = 50\n",
      "Number of features = 12288\n",
      "(209, 12288)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Flatten the data so that each (num_px, num_px, 3) image is represented numpy-array of shape (num_px * num_px * 3, 1) \n",
    "#\n",
    "train_set_x_flatten= np.reshape (train_set_x_orig, (m_train,-1), \"C\")\n",
    "test_set_x_flatten= np.reshape (test_set_x_orig, (m_test,-1), \"C\")\n",
    "m_train = train_set_x_flatten.shape[0]\n",
    "m_test = test_set_x_flatten.shape[0]\n",
    "n_features = train_set_x_flatten.shape[1]\n",
    "print (\"Flattened Training set size = \" + str(m_train))\n",
    "print (\"Flattened Training set shape = \" + str(train_set_x_flatten.shape));\n",
    "print (\"Flattened Test set size = \" + str(m_test))\n",
    "print (\"Number of features = \" + str(n_features))\n",
    "print (train_set_x_flatten.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum mean & std in training set = 134.2153110047847 and 81.44342276651373\n",
      "Maximum mean & std in training set after normalization = 2.407696583307905e-16 and 1.000000000000001\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Normalize the training and test data\n",
    "# After normalization, the mean value in each feature should be close to 0 and variance should be close to 1\n",
    "# Without normalization, Gradient Descent diverges right away for the same hyperparameters for this use case.\n",
    "#\n",
    "max_mu = np.max(np.mean(train_set_x_flatten, axis=0), axis=0)\n",
    "max_std = np.max(np.std(train_set_x_flatten, axis=0), axis=0)\n",
    "print (\"Maximum mean & std in training set = {} and {}\".format (max_mu, max_std));\n",
    "\n",
    "train_set_x_norm, train_mu, train_sigma = normalize_features(train_set_x_flatten) \n",
    "\n",
    "max_mu = np.max(np.mean(train_set_x_norm, axis=0), axis=0)\n",
    "max_std = np.max(np.std(train_set_x_norm, axis=0), axis=0)\n",
    "print (\"Maximum mean & std in training set after normalization = {} and {}\".format (max_mu, max_std));\n",
    "\n",
    "#\n",
    "# It is important to normalize the test data using the same mean & variance value used for training set\n",
    "#\n",
    "test_set_x_norm = (test_set_x_flatten - train_mu)/train_sigma\n",
    "\n",
    "#\n",
    "# Constants defining the number of layers in the model\n",
    "# Note that the number of units in the first layer must be equal to the number of features in a training sample\n",
    "# The first layer is not counted as a hidden layer. \n",
    "# Weights & parameters are computed only for the hidden layers.\n",
    "#\n",
    "layers_dims = [12288, 20, 7, 5, 1] \n",
    "nlayers = len(layers_dims)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters with 'he' method\n",
      "Cost after iteration 0: 1.8495063747767715\n",
      "Cost after iteration 100: 0.5101219913122284\n",
      "Cost after iteration 200: 0.47433306464841507\n",
      "Cost after iteration 300: 0.4098983036881122\n",
      "Cost after iteration 400: 0.36462625553353895\n",
      "Cost after iteration 500: 0.3358929083921332\n",
      "Cost after iteration 600: 0.31944958957158026\n",
      "Cost after iteration 700: 0.31137654877909626\n",
      "Cost after iteration 800: 0.30792835465114415\n",
      "Cost after iteration 900: 0.30665962836655053\n",
      "Cost after iteration 1000: 0.30623013267746413\n",
      "Cost after iteration 1100: 0.3061114012943544\n",
      "Cost after iteration 1200: 0.30606824712637754\n",
      "Cost after iteration 1300: 0.3060562548089629\n",
      "Cost after iteration 1400: 0.3060555461846977\n",
      "Cost after iteration 1500: 0.30605261852091187\n",
      "Cost after iteration 1600: 0.30605367704428876\n",
      "Cost after iteration 1700: 0.3060530695107565\n",
      "Cost after iteration 1800: 0.30605397659203715\n",
      "Cost after iteration 1900: 0.30605315975683783\n",
      "Cost after iteration 2000: 0.30608365538687027\n",
      "Cost after iteration 2100: 0.30313113980357087\n",
      "Cost after iteration 2200: 0.3029675035488141\n",
      "Cost after iteration 2300: 0.30295039637727167\n",
      "Cost after iteration 2400: 0.3029335640864838\n",
      "Cost after iteration 2500: 0.30291870064830695\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_norm.T, \\\n",
    "                           train_set_y, \\\n",
    "                           layers_dims, num_iterations = 2501, \\\n",
    "                           learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999, lambd = 0.01, \\\n",
    "                           init_method = \"he\", print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, nlayers, parameters):\n",
    "    m = X.shape[1]\n",
    "    probabilities, caches = L_model_forward(X, nlayers, parameters)\n",
    "    predictions = (probabilities > 0.5).astype(int)\n",
    "    print(\"Accuracy: \" + str(np.sum((predictions == Y)/m)))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.894736842105\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "train_predictions = predict(train_set_x_norm.T, train_set_y, nlayers, parameters)\n",
    "train_predictions = predict(test_set_x_norm.T, test_set_y, nlayers, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
