{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125085bd-0d23-49d0-abc7-c2c2ad3a6eed",
   "metadata": {},
   "source": [
    "# Train an MNIST model with TensorFlow\n",
    "\n",
    "MNIST is a widely-used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train a TensorFlow V2 model on MNIST model on SageMaker.\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook is meant to be run locally withing SageMaker studio. Of course, this is suitable only for debugging purposes when you are working with small datasets. Once you have determined that it runs well, the we can switch to the mode where we run training script on SageMaker infrastracture in a containerized environment.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [TensorFlow Estimator](#TensorFlow-Estimator)\n",
    "1. [Implement the training entry point](#Implement-the-training-entry-point)\n",
    "1. [Set hyperparameters](#Set-hyperparameters)\n",
    "1. [Set up channels for training and testing data](#Set-up-channels-for-training-and-testing-data)\n",
    "1. [Run the training script on SageMaker](#Run-the-training-script-on-SageMaker)\n",
    "1. [Inspect and store model data](#Inspect-and-store-model-data)\n",
    "1. [Test and debug the entry point before running the training container](#Test-and-debug-the-entry-point-before-running-the-training-container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207b3a75-a4a4-4e22-b9e0-fdf97a837082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagemaker version = 2.44.0\n",
      "S3 bucket for model artifacts = sagemaker-us-east-1-937351930975\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print (\"Sagemaker version = {}\".format (sagemaker.__version__))   \n",
    "print (\"S3 bucket for model artifacts = {}\".format (bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389f1475-7228-4357-abff-794173987869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the download of MNIST training/test data from 'sagemaker-sample-files' S3 bucket\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "#\n",
    "# Download training and testing data from a public S3 bucket\n",
    "#\n",
    "def download_from_s3(bucket, data_dir=\"./data\", train=True):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "sample_files_bucket = f\"sagemaker-sample-files\"\n",
    "download_from_s3 (sample_files_bucket, \"./data\", True)    # trainung data\n",
    "download_from_s3 (sample_files_bucket, \"./data\", False)   # test data\n",
    "\n",
    "print (\"Completed the download of MNIST training/test data from '{}' S3 bucket\".format(sample_files_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ad7331-d4f0-4444-88a7-e60eb691eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def load_mnist_data (data_dir, train):\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "        \n",
    "    with gzip.open(os.path.join(data_dir, images_file), \"rb\") as f_images:\n",
    "        images = np.frombuffer(f_images.read(), np.uint8, offset=16).reshape(-1, 28, 28)   \n",
    "        images = images.astype(np.float32)\n",
    "        \n",
    "    with gzip.open(os.path.join(data_dir, labels_file), \"rb\") as f_labels:\n",
    "        labels = np.frombuffer(f_labels.read(), np.uint8, offset=8).reshape(-1, 1)         \n",
    "\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27c10d4-e797-4c8a-9883-9aa1bb514a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis):\n",
    "    epsilon = np.finfo(float).eps\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True) + epsilon\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ae5238-aa9a-4e09-bb29-8c5fdf25ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size = (60000, 28, 28) and (60000, 1)\n",
      "Test data size = (10000, 28, 28) and (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "\n",
    "# \n",
    "# Load the training and test MNIST datasets and create Numpy array representation of the samples and labels\n",
    "#\n",
    "X_train, y_train = load_mnist_data(data_dir=data_dir, train=True)\n",
    "X_test, y_test = load_mnist_data(data_dir=data_dir, train=False)\n",
    "print (\"Training data size = {} and {}\".format (X_train.shape, y_train.shape))\n",
    "print (\"Test data size = {} and {}\".format (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c2ecc2-4264-4b3b-a652-c40ec85fe7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define hyperparameters for TensorFlow training\n",
    "#\n",
    "batch_size = 1024         # mini-batch size\n",
    "epochs = 5                # number of epochs of Gradient Descent\n",
    "learning_rate = 0.001     # initial learning rate used for Gradient Descent\n",
    "beta_1 = 0.9              # momentum parameter for Adam Optimizer\n",
    "beta_2 = 0.999            # rmsprop parameter for Adam Optimzier   \n",
    "dropout = 0.8             # rate used for dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f8d4a2-2c69-4339-824d-bfaa4487bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# https://www.tensorflow.org/guide/data\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# Using TensorFlow APIs, create mini-batches out of the training set by first shuffling it and then splitting it into mini-batches\n",
    "# tf.data.Dataset.from_tensor_slices: Creates a Dataset by slicing the given tensors along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension.\n",
    "# tf.data.Dataset.shuffle: Randomly shuffles the elements of this dataset by filling a buffer with given number of elements from the Dataset. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "# tf.data.Dataset.batch: Combines consecutive elements of this dataset into batches and returns a BatchDataset so that that we can iterate over the batches\n",
    "#\n",
    "def generate_mini_batches_tf (X, y, mini_batch_size = 128, seed = 0):\n",
    "    m = X.shape[0]    \n",
    "    mini_batches = (tf.data.Dataset.from_tensor_slices((X, y)).shuffle(m,seed).batch(mini_batch_size))\n",
    "    print (\"Number of mini-batches created = {}\".format (len(mini_batches)))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0caf06e4-0965-4e3c-a1a3-7cda822a65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create mini-batches out of the training set by first shuffling it and then splitting it into mini-batches\n",
    "# This is a DIY implementation of creating mini-batches from the original dataset\n",
    "# Random shuffling is done synchronously between X and Y such that after the shuffling the i-th sample in X corresponds to the i-the label in Y. \n",
    "# The shuffling step ensures that examples will be split randomly into different mini-batches.\n",
    "#\n",
    "import math\n",
    "\n",
    "def generate_mini_batches (X, Y, mini_batch_size = 128, seed = 0):\n",
    "    #\n",
    "    # Step 1: Shuffle (X, Y).\n",
    "    # Note that we should use a different seed in each epoch to ensure that the samples are shuffled in a different order each time\n",
    "    # Note that the array X passed into this function is a four dimensional array, after adding a dimension to account for the color channel.\n",
    "    #\n",
    "    m = X.shape[0]    \n",
    "    np.random.seed(seed)  \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    #\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case (last mini-batch < mini_batch_size)\n",
    "    #\n",
    "    mini_batches = []\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    start = 0;\n",
    "    end = 0;\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        start = k*mini_batch_size;\n",
    "        end = start + mini_batch_size;\n",
    "        mini_batch_X = shuffled_X [start:end, :, :, :];\n",
    "        mini_batch_Y = shuffled_Y [start:end, :];\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    #\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    #\n",
    "    if m % mini_batch_size != 0:\n",
    "        start = end;\n",
    "        end = m;\n",
    "        mini_batch_X = shuffled_X [start:end, :, :, :];\n",
    "        mini_batch_Y = shuffled_Y [start:end, :];\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    print (\"Number of mini-batches created = {}\".format (len(mini_batches)))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2114793e-8cc0-453b-bbe3-3ba05607c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define the layers of neural network \n",
    "#\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "#\n",
    "# To create a neural network, use tensorflow.keras.Model as the base class\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "# Once the model is created, you can perform the following tasks:\n",
    "# 1. config the model with losses and metrics with model.compile()\n",
    "# 2. train the model with model.fit()\n",
    "# 3. use the model to do prediction with model.predict()\n",
    "#\n",
    "class SmallConv(Model):\n",
    "    #\n",
    "    # When implementing a neural network by subclassing the Model class,\n",
    "    # one should define the layers in __init__() and should implement the model's forward pass in call() method\n",
    "    #    \n",
    "    def __init__(self, rate):\n",
    "        super(SmallConv, self).__init__()\n",
    "        initializer = HeNormal(seed=1)        \n",
    "        self.conv1 = Conv2D(32, 3, activation=\"relu\", kernel_initializer=initializer)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation=\"relu\", kernel_initializer=initializer)\n",
    "        self.dense2 = Dense(10, activation='linear', kernel_initializer=initializer)\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    #\n",
    "    # This method defines the model's forward propagation'\n",
    "    # We can optionally have a boolean argument that can be used to specify a different behavior in training and inference\n",
    "    # Here, we are using Dropout during training only. Dropout is an approach to regularization where a random number of units is selected in each hidden layer and their contributions simply dropped out while computing the output of that layer\n",
    "    #\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        if training:\n",
    "          x = self.dropout(x, training=training)        \n",
    "        return self.dense2(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d306b4b2-d159-42c9-ac03-553436e6de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "def train_builtin (X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate, beta_1, beta_2, dropout_rate):\n",
    "    #\n",
    "    # Normalize the training and test samples to mean 0 and std 1\n",
    "    # Note that we are normalizing across axis 1 & 2 of the samples.\n",
    "    # That means that we are normalizing across the (28x28) values for each sample and not across all samples\n",
    "    #\n",
    "    X_train, X_test = normalize(X_train, (1,2)), normalize(X_test, (1,2))\n",
    "    \n",
    "    #\n",
    "    # MNIST dataset comprises (28x28) B&W images\n",
    "    # Until now, we have been dealing with the training/test as a two-dimensional array (n_samples, 784)\n",
    "    # In order for it to be compatible with the Convolutional layer in the neural network, we have to reshape it as a (n_sample, 28, 28) array\n",
    "    # Additionally, we need to add a extra axis that represents the color channel\n",
    "    # Because most deep learning neural networks (TensorFlow Keras included) expect a separate channel for color\n",
    "    # numpy.expand_dims: Insert a new axis that will appear at the position specified by 'axis' in the expanded array shape.\n",
    "    #\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "    #\n",
    "    # Create the neural network model\n",
    "    #\n",
    "    model = SmallConv(dropout_rate)\n",
    "    model.compile(\n",
    "        loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2),\n",
    "        metrics = [SparseCategoricalAccuracy()]            \n",
    "    )\n",
    "    #\n",
    "    # Fit the model with the training set\n",
    "    # Here, we are letting TensorFlow handle all aspects of creating mini-batches and iterating over them in each epoch\n",
    "    #\n",
    "    print (\"Fitting the Model\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        shuffle = True,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs,\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    #\n",
    "    # Evaluate the model using the test set\n",
    "    #\n",
    "    print (\"Evaluating the Model\")    \n",
    "    model.evaluate(\n",
    "        X_test, y_test,\n",
    "        batch_size = batch_size,\n",
    "        verbose = 1        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8586834c-471f-4434-ba36-9b309ff09b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# When we are training with TensorFlow's builtin methods, all you need to do is create a model and then call model.compile() and model.fit()\n",
    "# What TensorFlow does when model.fit() is called is fully customizable.\n",
    "# A core principle of Keras is progressive disclosure of complexity. You should always be able to get into lower-level workflows in a gradual way\n",
    "#\n",
    "# Option #1\n",
    "# https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "# When you need to customize what fit() does, you should override the training step function, namely, train_step, of the Model class. \n",
    "# This is the function that is called by fit() for every batch of data. \n",
    "# You will then be able to call fit() as usual and it will be running your own learning algorithm.\n",
    "# \n",
    "# Option #2\n",
    "# https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "# Now, if you want very low-level control over training & evaluation, you should write your own training & evaluation loops from scratch. \n",
    "# In this implementation, we are following this option.\n",
    "#\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "def train_custom (X_train, y_train, \n",
    "                  X_test, y_test, \n",
    "                  epochs, batch_size, learning_rate, beta_1, beta_2, dropout_rate, n_batches = 10):\n",
    "    #\n",
    "    # Normalize the training and test samples to mean 0 and std 1\n",
    "    # Note that we are normalizing across axis 1 & 2 of the samples.\n",
    "    # That means that we are normalizing across the (28x28) values for each sample and not across all samples\n",
    "    #\n",
    "    X_train, X_test = normalize(X_train, (1,2)), normalize(X_test, (1,2))\n",
    "\n",
    "    #\n",
    "    # MNIST dataset comprises (28x28) B&W images\n",
    "    # Until now, we have been dealing with the training/test as a two-dimensional array (n_samples, 784)\n",
    "    # In order for it to be compatible with the Convolutional layer in the neural network, we have to reshape it as a (n_sample, 28, 28) array\n",
    "    # Additionally, we need to add a extra axis that represents the color channel\n",
    "    # Because most deep learning neural networks (TensorFlow Keras included) expect a separate channel for color\n",
    "    #\n",
    "    X, Xt = np.expand_dims(X_train, axis=3), np.expand_dims(X_test, axis=3)\n",
    "    y, yt = y_train, y_test\n",
    "    \n",
    "    #\n",
    "    # Create mini-batches for the test set. It is not necessary to do this if the test set is much smaller than the training set.\n",
    "    # Also, we will be using the test set only for inferencing. Hence, it would suffice to create these mini-batches just once and resuse them.\n",
    "    #\n",
    "    test_mini_batches = generate_mini_batches_tf (Xt, yt, batch_size)\n",
    "    #test_mini_batches = generate_mini_batches (Xt, yt, batch_size)\n",
    "\n",
    "    #\n",
    "    # Create the neural network model\n",
    "    # Note that we are compiling the model without specifying a loss function and an optimizer\n",
    "    # These can be instantiated separately and used in the custom training loop to have more granular control over the training algorithm\n",
    "    #\n",
    "    model = SmallConv(dropout_rate)\n",
    "    model.compile()\n",
    "    \n",
    "    #\n",
    "    # Define loss function and optimizer to be used in Gradient Descent calculations\n",
    "    #\n",
    "    loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)  \n",
    "    \n",
    "    #\n",
    "    # We can track both training and test loss/accuracy using built-in metrics and calling appropriate methods on them\n",
    "    # Call metric.update_state() after each batch\n",
    "    # Call metric.result() when you need to display the current value of the metric\n",
    "    # Call metric.reset_states() when you need to clear the state of the metric (typically at the end of an epoch)\n",
    "    #\n",
    "    train_metric_loss = Mean(name=\"train_loss\")\n",
    "    train_metric_accuracy = SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "    test_metric_loss = Mean(name=\"test_loss\")\n",
    "    test_metric_accuracy = SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
    "    \n",
    "    #\n",
    "    # You can use tf.function to make graphs out of your programs. \n",
    "    # It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. \n",
    "    # This will help you create performant and portable models, and it is required to use SavedModel.\n",
    "    # Here, we are encapsulating all the operations performed in a training step with a mini-batch of data\n",
    "    #\n",
    "    @tf.function\n",
    "    def train_step_optimized (X_batch, y_batch):\n",
    "        #\n",
    "        # Open a GradientTape to record the operations run during the forward pass, which enables auto-differentiation\n",
    "        # Run the forward pass of the layer and get the predictions (also called logits) from the model\n",
    "        # Operations that the layer applies to its inputs are going to be recorded on the GradientTape.    \n",
    "        # Compute the loss value for this mini-batch.\n",
    "        #\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model (X_batch, training=True)  \n",
    "            loss_value = loss_fn (y_batch, logits)\n",
    "\n",
    "        #\n",
    "        # Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n",
    "        # Thin one step encapsulates all of the complex back propagation in a neural network.\n",
    "        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n",
    "        # Update the training metric(s) so that we can later query them using result()\n",
    "        #\n",
    "        grads = tape.gradient (loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients (zip(grads, model.trainable_weights))\n",
    "        train_metric_loss.update_state (loss_value)\n",
    "        train_metric_accuracy.update_state (y_batch, logits)\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Here, we are encapsulating all the operations performed in a test step with a mini-batch of data\n",
    "    #\n",
    "    @tf.function    \n",
    "    def test_step_optimized (X_batch, y_batch):\n",
    "        logits = model (X_batch, training=False)  \n",
    "        loss_value = loss_fn (y_batch, logits)\n",
    "        test_metric_loss.update_state (loss_value)\n",
    "        test_metric_accuracy.update_state (y_batch, logits)  \n",
    "        return  \n",
    "    \n",
    "    #\n",
    "    # A custom training look follows these steps:\n",
    "    # 1. We open a for loop that iterates over epochs\n",
    "    # 2. For each epoch, we open a for loop that iterates over the dataset, in batches\n",
    "    # 3. For each batch, we open a GradientTape() scope\n",
    "    # 4. Inside this scope, we call the model (forward pass) and compute the loss\n",
    "    # 5. Outside the scope, we retrieve the gradients of the weights of the model with regard to the loss\n",
    "    # 6. Finally, we use the optimizer to update the weights of the model based on the gradients    \n",
    "    #\n",
    "    seed = 0\n",
    "    for i in range(epochs):\n",
    "        print(\"Start of epoch {}\".format(i+1))\n",
    "        \n",
    "        #\n",
    "        # Create minibatches for this epoch\n",
    "        # Increment the seed to reshuffle differently the dataset after each epoch\n",
    "        #\n",
    "        seed = seed + 1\n",
    "        train_mini_batches = generate_mini_batches (X, y, batch_size, seed)\n",
    "        #train_mini_batches = generate_mini_batches_tf (X, y, batch_size, seed)\n",
    "                \n",
    "        #\n",
    "        # Note that we would need to call reset_states() on our metrics between each epoch! \n",
    "        # Otherwise calling result() on the metric would return an average since the start of training instead of the per-epoch averages.\n",
    "        #\n",
    "        test_metric_accuracy.reset_states()\n",
    "        test_metric_loss.reset_states()\n",
    "        train_metric_accuracy.reset_states()\n",
    "        train_metric_loss.reset_states()        \n",
    "                   \n",
    "        step = 0\n",
    "        for (X_batch, y_batch) in train_mini_batches:        \n",
    "            step = step + 1\n",
    "            train_step_optimized(X_batch, y_batch)\n",
    "\n",
    "            #\n",
    "            # Log periodically based on the n_batches parameter\n",
    "            #\n",
    "            if step % n_batches == 0:\n",
    "                samples = (step + 1) * batch_size\n",
    "                accuracy = train_metric_accuracy.result()\n",
    "                loss = train_metric_loss.result()\n",
    "                print(\"Loss at step {} = {} after {} samples; Accuracy = {}\".format (step, loss, samples, accuracy))  \n",
    "                \n",
    "        #\n",
    "        # Run a validation against the test set at the end of each epoch\n",
    "        #\n",
    "        for (X_batch, y_batch) in test_mini_batches:   \n",
    "            test_step_optimized(X_batch, y_batch)\n",
    "                \n",
    "        #\n",
    "        # Print loss, accuracy metrics for this epoch for both training and test samples\n",
    "        #\n",
    "        print(\"Epoch {}; Loss = {}; Training Accuracy {}; Test Accuracy {}\".format ((i+1), \n",
    "                                                                                    train_metric_loss.result(), \n",
    "                                                                                    train_metric_accuracy.result(), \n",
    "                                                                                    test_metric_accuracy.result()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4fd2386-f860-479d-bab6-4f7ee05f93ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Model\n",
      "[2023-04-25 16:41:36.927 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-25 16:41:37.212 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:57 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 1/5\n",
      "59/59 [==============================] - 17s 281ms/step - loss: 2.1080 - sparse_categorical_accuracy: 0.7178\n",
      "Epoch 2/5\n",
      "59/59 [==============================] - 17s 289ms/step - loss: 0.2761 - sparse_categorical_accuracy: 0.9214\n",
      "Epoch 3/5\n",
      "59/59 [==============================] - 17s 283ms/step - loss: 0.1958 - sparse_categorical_accuracy: 0.9452\n",
      "Epoch 4/5\n",
      "59/59 [==============================] - 17s 283ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "59/59 [==============================] - 17s 284ms/step - loss: 0.1294 - sparse_categorical_accuracy: 0.9638\n",
      "Evaluating the Model\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.1277 - sparse_categorical_accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "dropout = 0.0\n",
    "train_builtin (X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate, beta_1, beta_2, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2df20536-f1b0-4296-8733-2dbbf77f8e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mini-batches created = 10\n",
      "Start of epoch 1\n",
      "Number of mini-batches created = 59\n",
      "Loss at step 10 = 9.448932647705078 after 11264 samples; Accuracy = 0.31718748807907104\n",
      "Loss at step 20 = 5.545870304107666 after 21504 samples; Accuracy = 0.46479493379592896\n",
      "Loss at step 30 = 4.031325817108154 after 31744 samples; Accuracy = 0.5448567867279053\n",
      "Loss at step 40 = 3.214946746826172 after 41984 samples; Accuracy = 0.6015869379043579\n",
      "Loss at step 50 = 2.702432632446289 after 52224 samples; Accuracy = 0.6426953077316284\n",
      "Epoch 1; Loss = 2.384625196456909; Training Accuracy 0.6667166948318481; Test Accuracy 0.8177000284194946\n",
      "Start of epoch 2\n",
      "Number of mini-batches created = 59\n",
      "Loss at step 10 = 0.5760658383369446 after 11264 samples; Accuracy = 0.8135741949081421\n",
      "Loss at step 20 = 0.5615295767784119 after 21504 samples; Accuracy = 0.8194824457168579\n",
      "Loss at step 30 = 0.5451758503913879 after 31744 samples; Accuracy = 0.8213866949081421\n",
      "Loss at step 40 = 0.5272332429885864 after 41984 samples; Accuracy = 0.824902355670929\n",
      "Loss at step 50 = 0.5117594003677368 after 52224 samples; Accuracy = 0.8275195360183716\n",
      "Epoch 2; Loss = 0.497903436422348; Training Accuracy 0.8291166424751282; Test Accuracy 0.8547000288963318\n",
      "Start of epoch 3\n",
      "Number of mini-batches created = 59\n",
      "Loss at step 10 = 0.3480580747127533 after 11264 samples; Accuracy = 0.8848632574081421\n",
      "Loss at step 20 = 0.33798113465309143 after 21504 samples; Accuracy = 0.901562511920929\n",
      "Loss at step 30 = 0.31591376662254333 after 31744 samples; Accuracy = 0.9108723998069763\n",
      "Loss at step 40 = 0.29487571120262146 after 41984 samples; Accuracy = 0.9177001714706421\n",
      "Loss at step 50 = 0.27766966819763184 after 52224 samples; Accuracy = 0.9224023222923279\n",
      "Epoch 3; Loss = 0.2673037052154541; Training Accuracy 0.9248166680335999; Test Accuracy 0.9449999928474426\n",
      "Start of epoch 4\n",
      "Number of mini-batches created = 59\n",
      "Loss at step 10 = 0.18700726330280304 after 11264 samples; Accuracy = 0.9476562738418579\n",
      "Loss at step 20 = 0.18337410688400269 after 21504 samples; Accuracy = 0.9486328363418579\n",
      "Loss at step 30 = 0.17885242402553558 after 31744 samples; Accuracy = 0.9495442509651184\n",
      "Loss at step 40 = 0.17471231520175934 after 41984 samples; Accuracy = 0.9507080316543579\n",
      "Loss at step 50 = 0.17307418584823608 after 52224 samples; Accuracy = 0.9508593678474426\n",
      "Epoch 4; Loss = 0.17062833905220032; Training Accuracy 0.9512666463851929; Test Accuracy 0.9560999870300293\n",
      "Start of epoch 5\n",
      "Number of mini-batches created = 59\n",
      "Loss at step 10 = 0.14345377683639526 after 11264 samples; Accuracy = 0.960742175579071\n",
      "Loss at step 20 = 0.14337456226348877 after 21504 samples; Accuracy = 0.9615722894668579\n",
      "Loss at step 30 = 0.14063487946987152 after 31744 samples; Accuracy = 0.9612630009651184\n",
      "Loss at step 40 = 0.13829191029071808 after 41984 samples; Accuracy = 0.9612792730331421\n",
      "Loss at step 50 = 0.137321338057518 after 52224 samples; Accuracy = 0.9616601467132568\n",
      "Epoch 5; Loss = 0.1361052691936493; Training Accuracy 0.9619500041007996; Test Accuracy 0.9616000056266785\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "dropout = 0.0\n",
    "train_custom (X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate, beta_1, beta_2, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b4bd4-97d2-411d-b85d-59c553f152fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
